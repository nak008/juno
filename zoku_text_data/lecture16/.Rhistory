install.packages("quantmod", "forecast", "tseries")
library(quantmod)
install.packages("quantmod", "forecast", "tseries")
install.packages("quantmod")
install.packages("tseries")
library(quantmod)
library(forecast)
library(tseries)
install.packages("forecast")
library(forecast)
library(tseries)
library(quantmod)
data <- list(N=length(X), Y=Y)
plot(Y~X, type="l")
X <- 1961:1990
Y <- c(4.71, 7.70, 7.97, 8.35, 5.70,
7.33, 3.10, 4.98, 3.75, 3.35,
1.84, 3.28, 2.77, 2.72, 2.54,
3.23, 2.45, 1.90, 2.56, 2.12,
1.78, 3.18, 2.64, 1.86, 1.69,
0.81, 1.02, 1.40, 1.31, 1.57)
data <- list(N=length(X), Y=Y)
plot(Y~X, type="l")
library(rstan)
install.packages("rstan")
library(rstan)
rstan_options(auto_write=TRUE)
options(mc.cores = parallel::detectCores())
#ローカルレベルモデル
localLevelModel <- 'data{
int<lower = 0> N;
real<lower= 0> Y[N];
}
parameters{
real alpha[N];
real<lower = 0>s_Y;
real<lower = 0>s_a;
}
model {
for(i in 1:N)
Y[i] ~ normal(alpha[i], s_Y);
for(i in 2:N)
alpha[i] ~ normal(alpha[i-1], s_a);
}
'
#MCMC
stanlocal <-stan_model(model_code = localLevelModel)
library(rstan)
rstan_options(auto_write=TRUE)
options(mc.cores = parallel::detectCores())
#ローカルレベルモデル
localLevelModel <- 'data{
int<lower = 0> N;
real<lower= 0> Y[N];
}
parameters{
real alpha[N];
real<lower = 0>s_Y;
real<lower = 0>s_a;
}
model {
for(i in 1:N)
Y[i] ~ normal(alpha[i], s_Y);
for(i in 2:N)
alpha[i] ~ normal(alpha[i-1], s_a);
}
'
#MCMC
stanlocal <-stan_model(model_code = localLevelModel)
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
#MCMC
stanlocal <-stan_model(model_code = localLevelModel)
library(rstan)
rstan_options(auto_write=TRUE)
options(mc.cores = parallel::detectCores())
#ローカルレベルモデル
localLevelModel <- 'data{
int<lower = 0> N;
real<lower= 0> Y[N];
}
parameters{
real alpha[N];
real<lower = 0>s_Y;
real<lower = 0>s_a;
}
model {
for(i in 1:N)
Y[i] ~ normal(alpha[i], s_Y);
for(i in 2:N)
alpha[i] ~ normal(alpha[i-1], s_a);
}
'
#MCMC
stanlocal <-stan_model(model_code = localLevelModel)
p<-1/2
n<-10
120*p^3*(1-p)^7
120*p^x*(1-p)^(n-x)
x<-3
120*p^x*(1-p)^(n-x)
x<-4
dbinom(x=x, size=n, prob=p)
p<-1/2
n<-10
x<-3
dbinom(x=x, size=n, prob=p)
x<-0:10
x
#これで上と同じ
dbinom(x=x, size=n, prob=p)
#これで上と同じ
plot(dbinom(x=x, size=n, prob=p))
p<-1/6
#これで上と同じ
plot(dbinom(x=x, size=n, prob=p))
p<-4/6
#これで上と同じ
plot(dbinom(x=x, size=n, prob=p))
p<-9/10
#これで上と同じ
plot(dbinom(x=x, size=n, prob=p))
rbinom(n=5, size=n, prob=p)
p<-1/2
n<-10
rbinom(n=5, size=n, prob=p)
#これで上と同じ
#d binom(二項分布)
plot(dbinom(x=x, size=n, prob=p))
setwd("C:/Users/taror/iCloudDrive/sugakubunka/集団授業/zokushokyu/2021-04/lecture16")
train <-read.csv("./data/xor_demo_train.csv",
fileEncoding = "utf-8"
)
head(train)
plot(train$x1, train$x2, col=train$y)
x1_min <- min(train$x1)
x1_max <- min(train$x1)
x2_min <- min(train$x2)
x2_max <- max(train$x2)
library(kernlab)
install.packages("kernlab")
library(kernlab)
result <- ksvm(x = y ~ x1+ x2,
data = trian_scaled,
type = "C-svc",
C = 1.0,
kernel = "rbfdot",
kpar = list(sigma=1.0))
train_scaled <- data.frame(x1=x2, x2=x2, y=train$y)
x1_min <- min(train$x1)
x1_max <- min(train$x1)
x2_min <- min(train$x2)
x2_max <- max(train$x2)
x1 <- (train$x1 - x1_min)/(x1_max-x1_min)
x2 <- (train$x2 - x2_min)/(x2_max-x2_min)
train_scaled <- data.frame(x1=x2, x2=x2, y=train$y)
head(train_scaled, n=5)
result <- ksvm(x = y ~ x1+ x2,
data = trian_scaled,
type = "C-svc",
C = 1.0,
kernel = "rbfdot",
kpar = list(sigma=1.0))
result <- ksvm(x = y ~ x1+ x2,
data = train_scaled,
type = "C-svc",
C = 1.0,
kernel = "rbfdot",
kpar = list(sigma=1.0))
result <- ksvm(x = y ~ x1+ x2,
data = train_scaled,
type = "C-svc",
C = 1.0,
kernel = "rbfdot",
kpar = list(sigma=1.0))
result
xx1 <- seq(-3, 3, 0.05)
xx2 <- seq(-3, 3, 0.05)
meshgrid <- expand.grid(xx1,xx2)
names(meshgrid) <- c("x1","x2")
head(meshgrid, n=5)
meshgrid <- expand.grid(xx1,xx2)
names(meshgrid) <- c("x1","x2")
head(meshgrid, n=5)
x1 <- (test$x1 - x1_min)/(x1_max-x1_min)
x2 <- (test$x2 - x2_min)/(x2_max-x2_min)
test_scaled <- data.frame(x1=x2, x2=x2, y=test$y)
head(test_scaled, n=5)
test <-read.csv("./data/xor_demo_test.csv",
fileEncoding = "utf-8"
)
x1 <- (test$x1 - x1_min)/(x1_max-x1_min)
x2 <- (test$x2 - x2_min)/(x2_max-x2_min)
test_scaled <- data.frame(x1=x2, x2=x2, y=test$y)
head(test_scaled, n=5)
pred_test <- predict(result, test_scaled, type = "response")
table(test$y, pred_test)
test_scaled <- data.frame(x1=x1, x2=x2, y=test$y)
head(test_scaled, n=5)
pred_test <- predict(result, test_scaled, type = "response")
table(test$y, pred_test)
x1 <- (test$x1 - x1_min)/(x1_max-x1_min)
x2 <- (test$x2 - x2_min)/(x2_max-x2_min)
test_scaled <- data.frame(x1=x1, x2=x2, y=test$y)
head(test_scaled, n=5)
x1 <- (test$x1 - x1_min)/(x1_max-x1_min)
x1
test_scaled <- data.frame(x1=x1, x2=x2, y=test$y)
head(test_scaled, n=5)
(x1_max-x1_min)
train <-read.csv("./data/xor_demo_train.csv",
fileEncoding = "utf-8")
#ランダムフォレストの実行
result <- ranger(formula=label~x+y,
data=dat,
mtry=1,
max.depth=2,
min.node.size=20,
write.forest=TRUE,
importance = "impurity",
classification = TRUE)
#ランダムフォレストを実行するためのパッケージ
library(ranger)
#ランダムフォレストの実行
result <- ranger(formula=label~x+y,
data=dat,
mtry=1,
max.depth=2,
min.node.size=20,
write.forest=TRUE,
importance = "impurity",
classification = TRUE)
setwd("C:/Users/taror/iCloudDrive/sugakubunka/集団授業/zokushokyu/2021-04/lecture15")
#データの作成
dat <- read.csv("./data/split_demo.csv",fileEncoding="utf-8")
#ランダムフォレストの実行
result <- ranger(formula=label~x+y,
data=dat,
mtry=1,
max.depth=2,
min.node.size=20,
write.forest=TRUE,
importance = "impurity",
classification = TRUE)
result
setwd("C:/Users/taror/iCloudDrive/sugakubunka/集団授業/zokushokyu/2021-04/lecture16")
train <-read.csv("./data/xor_demo_train.csv",
fileEncoding = "utf-8")
head(train, n=5)
str(train)
train$y <- as.factor(train$y)
str(train)
plot(train$x1, train$x2, col=train$y)
plot(train$x1, train$x2, col=train$y)
x1_min<-min(train$x1)
x1_max<-max(train$x1)
x2_min<-min(train$x2)
x2_max<-max(train$x2)
x1 <- (train$x1-x1_min)/(x1_max -x1_min)
x2 <- (train$x2-x2_min)/(x2_max -x2_min)
train_scaled <- data.frame(x1=x1, x2=x2, y=train$y)
summary(train_scaled)
#SVMの実行
library(kernlab)
library(kernlab)
result <- ksvm(x= y~x1+x2,
data = train_scaled,#与えるデータは正規化した方
type = "C-svc", #分類の仕方を指定
C=1.0, #決定境界の複雑さが変わるハイパーパラメータ
kernel = "rbfdot", #決定境界の形に関わるもの
kpar = list(sigma=1.0) #今回はrbfdotのハイパーパラメータ
)
result
#結果の確認
xx1 <- seq(-3,3, 0.05)
xx2 <- seq(-3,3, 0.05)
xx1
length(xx1)
121*121
data.frame(x1=xx1,x2=xx2)
meshgrid <- expand.grid(xx1, xx2)
meshgrid
names(meshgrid) <- c("x1", "x2")
head(meshgrid)
head(meshgrid, n=20)
x2 <- (meshgrid$x2-x2_min)/(x2_max-x2_min)
#正規化
x1 <- (meshgrid$x1-x1_min)/(x1_max-x1_min)
x2 <- (meshgrid$x2-x2_min)/(x2_max-x2_min)
meshgrid_scaled <- data.frame(x1=x1, x2=x2)
head(meshgrid_scaled, n=5)
plot(meshgrid_scaled$x1, meshgrid_scaled$x2)
#各点に対して予測を行う
pred <- predict(result, meshgrid_scaled, type="decision")
pred
#各点に対して予測を行う
predict(result, meshgrid_scaled, type="response")
pred <- predict(result, meshgrid_scaled, type="decision")
pred
contour(xx1, xx2, array(pred, dim=c(length(xx1), length(xx2)))
contour(xx1,
xx2,
array(pred, dim=c(length(xx1), length(xx2))
)
)
contour(xx1,
xx2,
array(pred, dim=c(length(xx1), length(xx2))
)
)
result <- ksvm(x= y~x1+x2,
data = train_scaled,#与えるデータは正規化した方
type = "C-svc", #分類の仕方を指定
C=1.0, #決定境界の複雑さが変わるハイパーパラメータ
kernel = "rbfdot", #決定境界の形に関わるもの
kpar = list(sigma=100.0) #今回はrbfdotのハイパーパラメータ
)
#各点に対して予測を行う
predict(result, meshgrid_scaled, type="response")#これは予測ラベルを返す
pred <- predict(result, meshgrid_scaled, type="decision")
#スコアの等高線
contour(xx1,
xx2,
array(pred, dim=c(length(xx1), length(xx2))
)
)
result <- ksvm(x= y~x1+x2,
data = train_scaled,#与えるデータは正規化した方
type = "C-svc", #分類の仕方を指定
C=1.0, #決定境界の複雑さが変わるハイパーパラメータ
kernel = "rbfdot", #決定境界の形に関わるもの
kpar = list(sigma=0.0001) #今回はrbfdotのハイパーパラメータ
)
#各点に対して予測を行う
predict(result, meshgrid_scaled, type="response")#これは予測ラベルを返す
pred <- predict(result, meshgrid_scaled, type="decision")
#スコアの等高線
contour(xx1,
xx2,
array(pred, dim=c(length(xx1), length(xx2))
)
)
result <- ksvm(x= y~x1+x2,
data = train_scaled,#与えるデータは正規化した方
type = "C-svc", #分類の仕方を指定
C=100.0, #決定境界の複雑さが変わるハイパーパラメータ
kernel = "rbfdot", #決定境界の形に関わるもの
kpar = list(sigma=1.0) #今回はrbfdotのハイパーパラメータ
)
result
#各点に対して予測を行う
predict(result, meshgrid_scaled, type="response")#これは予測ラベルを返す
pred <- predict(result, meshgrid_scaled, type="decision")
#スコアの等高線
contour(xx1,
xx2,
array(pred, dim=c(length(xx1), length(xx2))
)
)
head(test, n=5)
test <-read.csv("./data/xor_demo_test.csv",
fileEncoding = "utf-8")
head(test, n=5)
str(train)
#min-max正規化
x1 <- (test$x1-x1_min)/(x1_max -x1_min)
x2 <- (test$x2-x2_min)/(x2_max -x2_min)
test_scaled <- data.frame(x1=x1, x2=x2, y=train$y)
summary(test_scaled)
pred_test <- predict(result, test_scaled, type="response")
pred_test
plot(test$x1, test$x2, col=test$y)
table(test$y, pred_test)
length(test$y)
length(pred_test)
pred_test <- predict(result, test_scaled, type="response")
length(pred_test)
summary(test_scaled)
test_scaled <- data.frame(x1=x1, x2=x2, y=test$y)
summary(test_scaled)
pred_test <- predict(result, test_scaled, type="response")
pred_test
length(test$y)
length(pred_test)
table(test$y, pred_test)
summary(test_scaled)
test <-read.csv("./data/xor_demo_test.csv",
fileEncoding = "utf-8")
head(test, n=5)
test$y <- as.factor(test$y)
str(test)
plot(test$x1, test$x2, col=test$y)
#min-max正規化
x1 <- (test$x1-x1_min)/(x1_max -x1_min)
x2 <- (test$x2-x2_min)/(x2_max -x2_min)
test_scaled <- data.frame(x1=x1, x2=x2, y=test$y)
summary(test_scaled)
pred_test <- predict(result, test_scaled, type="response")
pred_test
table(test$y, pred_test)
summary(test_scaled)
table(test$y, pred_test)
pred_train <- predict(result, train_scaled, type="response")
table(train$y, pred_train)
265/300
table(test$y, pred_test)
265/300
table(test$y, pred_test)
table(test$y, pred_test)
table(test$y, pred_test)
82/100
